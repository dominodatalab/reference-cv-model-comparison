{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b9e24b-35e9-4ab6-af35-a7a1ca89c749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.3.1+cu121 torchvision==0.18.1+cu121 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb50405-cf71-47af-a7ce-1e0adf2a92df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade onnxruntime-gpu==1.18.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4459ea2-2742-4cad-87c5-a9b9f5cfd684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36964526-ba0c-4b48-8765-8edb146d3a61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, onnxruntime as ort\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # or unset it: os.environ.pop(\"CUDA_VISIBLE_DEVICES\", None)\n",
    "\n",
    "# Must be the GPU build:\n",
    "# pip install --upgrade onnxruntime-gpu==1.18.0   # pick version matching your CUDA 12.x stack\n",
    "\n",
    "prov = ort.get_available_providers()\n",
    "print(prov)\n",
    "assert \"CUDAExecutionProvider\" in prov, f\"ORT not GPU-enabled. Providers={prov}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d38dd6-3f28-460c-b010-c38e13305fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets_dir = os.environ['DOMINO_DATASETS_DIR']\n",
    "project_ds_folder = os.environ['DOMINO_PROJECT_NAME'] \n",
    "\n",
    "download_base_folder=f\"{datasets_dir}/{project_ds_folder}\"\n",
    "models_folder = \"models\"\n",
    "yolo_model_name=\"yolov8n\"\n",
    "\n",
    "yolo_onnx_file_name=f\"{yolo_model_name}.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae6ad0-baf5-4f36-afc9-d09248480575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adds: log entire Ultralytics run dir to MLflow, then delete it.\n",
    "# Writes the run under /tmp/ultra_runs/<name>, logs it, removes it.\n",
    "\n",
    "import os, time, yaml, math, random, json, shutil, statistics as stats\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "    _HAS_MLFLOW = True\n",
    "except Exception:\n",
    "    _HAS_MLFLOW = False\n",
    "\n",
    "\n",
    "def _ensure_dir(p: Path) -> Path:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "\n",
    "def _latency_benchmark(model: YOLO, image_paths: List[Path], imgsz: int, device: str) -> Dict[str, Any]:\n",
    "    if not image_paths:\n",
    "        return {\"p50_ms\": None, \"p90_ms\": None, \"p99_ms\": None, \"mean_ms\": None, \"count\": 0}\n",
    "    lat_ms = []\n",
    "    _ = model.predict(source=str(image_paths[0]), imgsz=imgsz, device=device, verbose=False)  # warmup\n",
    "    for p in image_paths:\n",
    "        t0 = time.perf_counter()\n",
    "        _ = model.predict(source=str(p), imgsz=imgsz, device=device, verbose=False)\n",
    "        lat_ms.append((time.perf_counter() - t0) * 1000.0)\n",
    "    lat_sorted = sorted(lat_ms)\n",
    "    def pct(v, q):\n",
    "        idx = min(len(v)-1, max(0, int(math.ceil(q*len(v))-1)))\n",
    "        return v[idx]\n",
    "    return {\n",
    "        \"p50_ms\": pct(lat_sorted, 0.50),\n",
    "        \"p90_ms\": pct(lat_sorted, 0.90),\n",
    "        \"p99_ms\": pct(lat_sorted, 0.99),\n",
    "        \"mean_ms\": float(stats.mean(lat_sorted)),\n",
    "        \"count\": len(lat_sorted),\n",
    "        \"raw_ms\": lat_ms,\n",
    "    }\n",
    "\n",
    "\n",
    "def ensure_mlflow_experiment(experiment_name: str) -> int:\n",
    "    \"\"\"\n",
    "    Ensure an MLflow experiment with the given name exists.\n",
    "    If it does not, create it. Then set it as the current experiment.\n",
    "\n",
    "    Args:\n",
    "        experiment_name: Name of the experiment\n",
    "        artifact_location: Optional path or URI where artifacts will be stored\n",
    "\n",
    "    Returns:\n",
    "        experiment_id (int)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        exp = mlflow.get_experiment_by_name(experiment_name)\n",
    "        if exp is None:\n",
    "            exp_id = mlflow.create_experiment(\n",
    "                experiment_name\n",
    "            )\n",
    "        else:\n",
    "            exp_id = exp.experiment_id\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        return exp_id\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to ensure experiment {experiment_name}: {e}\")\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    base_path: str,\n",
    "    model_path: str,\n",
    "    imgsz: int = 640,\n",
    "    device: str = \"cpu\",\n",
    "    limit_images: int = 1000,\n",
    "    subset_seed: int = 0,\n",
    "    experiment_name:str = None\n",
    ") -> Dict[str, Any]:\n",
    "    base = Path(base_path)\n",
    "    img_dir = base / \"images\" / \"val2017\"\n",
    "    if not img_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing image dir: {img_dir}\")\n",
    "\n",
    "    artifacts = _ensure_dir(base / \"artifacts\")\n",
    "    plots_dir = _ensure_dir(artifacts / \"plots\")\n",
    "    metrics_dir = _ensure_dir(artifacts / \"metrics\")\n",
    "    config_dir = _ensure_dir(artifacts / \"config\")\n",
    "\n",
    "    all_imgs = sorted([p for p in img_dir.glob(\"*.jpg\")] +\n",
    "                      [p for p in img_dir.glob(\"*.jpeg\")] +\n",
    "                      [p for p in img_dir.glob(\"*.png\")])\n",
    "    if not all_imgs:\n",
    "        raise RuntimeError(f\"No images found under {img_dir}\")\n",
    "    rng = random.Random(subset_seed)\n",
    "    rng.shuffle(all_imgs)\n",
    "    sub_imgs = all_imgs[:min(limit_images, len(all_imgs))]\n",
    "\n",
    "    subset_list = artifacts / \"val_subset.txt\"\n",
    "    with open(subset_list, \"w\") as f:\n",
    "        for p in sub_imgs:\n",
    "            f.write(str(p.resolve()) + \"\\n\")\n",
    "\n",
    "    data_config = {\n",
    "        'path': str(base),\n",
    "        'train': 'images/val2017',\n",
    "        'val': str(subset_list),\n",
    "        'names': list(range(80))\n",
    "    }\n",
    "    yaml_path = base / \"coco_val_subset.yaml\"\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.dump(data_config, f)\n",
    "\n",
    "    model = YOLO(model_path, task=\"detect\")\n",
    "\n",
    "    # Force Ultralytics run dir into /tmp, give it a stable name\n",
    "    tmp_project = Path(\"/tmp/ultra_runs\")\n",
    "    run_name = f\"val_{Path(model_path).stem}_subset{len(sub_imgs)}_\" \\\n",
    "           f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    val_res = model.val(\n",
    "        data=str(yaml_path),\n",
    "        imgsz=imgsz,\n",
    "        device=device,\n",
    "        save_json=False,\n",
    "        verbose=False,\n",
    "        project=str(tmp_project),   # <- /tmp base\n",
    "        name=run_name,              # <- folder name\n",
    "        exist_ok=True               # <- don’t auto-increment\n",
    "        \n",
    "    )\n",
    "    save_dir = Path(val_res.save_dir)  # /tmp/ultra_runs/<run_name>\n",
    "\n",
    "    metrics = {\n",
    "        \"map\": float(val_res.box.map),\n",
    "        \"ap50\": float(val_res.box.map50),\n",
    "        \"ap75\": float(val_res.box.map75),\n",
    "        \"mean_precision\": float(val_res.box.mp),\n",
    "        \"mean_recall\": float(val_res.box.mr),\n",
    "        \"evaluated_images\": len(sub_imgs)\n",
    "    }\n",
    "    (metrics_dir / \"headline.json\").write_text(json.dumps(metrics, indent=2))\n",
    "\n",
    "    latency = _latency_benchmark(model, sub_imgs[:min(100, len(sub_imgs))], imgsz=imgsz, device=device)\n",
    "    (metrics_dir / \"latency.json\").write_text(json.dumps({k: v for k, v in latency.items() if k != \"raw_ms\"}, indent=2))\n",
    "\n",
    "    if latency.get(\"raw_ms\"):\n",
    "        plt.figure()\n",
    "        plt.hist(latency[\"raw_ms\"], bins=20)\n",
    "        plt.xlabel(\"Latency (ms)\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(\"Per-image latency (batch=1)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plots_dir / \"latency_hist.png\")\n",
    "        plt.close()\n",
    "\n",
    "    eval_cfg = {\n",
    "        \"imgsz\": imgsz,\n",
    "        \"device\": device,\n",
    "        \"subset_seed\": subset_seed,\n",
    "        \"limit_images\": limit_images,\n",
    "        \"subset_list\": str(subset_list),\n",
    "        \"model_path\": str(model_path),\n",
    "        \"ultralytics_save_dir\": str(save_dir),\n",
    "    }\n",
    "    (config_dir / \"eval.json\").write_text(json.dumps(eval_cfg, indent=2))\n",
    "\n",
    "    if _HAS_MLFLOW:\n",
    "        if experiment_name:\n",
    "            ensure_mlflow_experiment(experiment_name)\n",
    "        try:\n",
    "            with mlflow.start_run(run_name=run_name):\n",
    "                mlflow.log_params({\n",
    "                    \"imgsz\": imgsz, \"device\": device,\n",
    "                    \"limit_images\": len(sub_imgs), \"subset_seed\": subset_seed\n",
    "                })\n",
    "                mlflow.log_metrics({\n",
    "                    \"map\": metrics[\"map\"], \"ap50\": metrics[\"ap50\"], \"ap75\": metrics[\"ap75\"],\n",
    "                    \"mean_precision\": metrics[\"mean_precision\"], \"mean_recall\": metrics[\"mean_recall\"],\n",
    "                    \"latency_p50_ms\": latency.get(\"p50_ms\") or 0.0,\n",
    "                    \"latency_p90_ms\": latency.get(\"p90_ms\") or 0.0,\n",
    "                    \"latency_p99_ms\": latency.get(\"p99_ms\") or 0.0,\n",
    "                    \"latency_mean_ms\": latency.get(\"mean_ms\") or 0.0,\n",
    "                })\n",
    "                # Log our structured artifacts\n",
    "                mlflow.log_artifact(str(metrics_dir / \"headline.json\"))\n",
    "                mlflow.log_artifact(str(metrics_dir / \"latency.json\"))\n",
    "                mlflow.log_artifact(str(config_dir / \"eval.json\"))\n",
    "                mlflow.log_artifact(str(artifacts / \"val_subset.txt\"))\n",
    "                if (plots_dir / \"latency_hist.png\").exists():\n",
    "                    mlflow.log_artifact(str(plots_dir / \"latency_hist.png\"))\n",
    "                # Log the full Ultralytics run directory (results.csv/png, confusion matrix, samples, etc.)\n",
    "                if save_dir.exists():\n",
    "                    mlflow.log_artifacts(str(save_dir), artifact_path=\"ultralytics_run\")\n",
    "        finally:\n",
    "            # Always clean up the /tmp run directory\n",
    "            if save_dir.exists():\n",
    "                shutil.rmtree(save_dir, ignore_errors=True)\n",
    "            # Optional: prune empty parent\n",
    "            parent = tmp_project\n",
    "            if parent.exists() and not any(parent.iterdir()):\n",
    "                shutil.rmtree(parent, ignore_errors=True)\n",
    "\n",
    "    else:\n",
    "        # If MLflow isn’t available, still clean up /tmp run dir\n",
    "        if save_dir.exists():\n",
    "            shutil.rmtree(save_dir, ignore_errors=True)\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03455c5b-60c5-4afa-b281-ef37e790fed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = f\"{download_base_folder}/{models_folder}/{yolo_onnx_file_name}\"\n",
    "print(model_path)\n",
    "base_folder=f\"{download_base_folder}/coco\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4edffa-0a41-4389-b471-ab31f0c9ae47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "domino_user_name = os.environ['DOMINO_USER_NAME']\n",
    "experiment_name=f\"cv-comparison-{domino_user_name}\"\n",
    "metrics = evaluate_model(f\"{download_base_folder}/coco\",model_path,limit_images=50,experiment_name=experiment_name,device=\"0\")\n",
    "metrics = evaluate_model(f\"{download_base_folder}/coco\",model_path,limit_images=50,experiment_name=experiment_name,device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd6862-137b-4a0e-ab99-ca31adfd9b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
