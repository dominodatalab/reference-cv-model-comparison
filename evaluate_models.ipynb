{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4459ea2-2742-4cad-87c5-a9b9f5cfd684",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
      "View Ultralytics Settings with 'yolo settings' or at '/home/domino/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178637ec-ddb0-4dd9-997b-109aac296b0d",
   "metadata": {},
   "source": [
    "#### Purpose\n",
    "\n",
    "Ensure that ONNX Runtime is GPU-enabled by verifying the presence of the CUDAExecutionProvider. This is essential for leveraging GPU acceleration during model inference. If CUDA is not available, the code raises an error to avoid unintentional CPU fallback.\n",
    "\n",
    "#### Context\n",
    "\n",
    "Use this check before running inference-heavy workloads in computer vision pipelines. It's particularly relevant when running in cloud environments or containers, where GPU access may not be guaranteed by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36964526-ba0c-4b48-8765-8edb146d3a61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'AzureExecutionProvider', 'CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import os, onnxruntime as ort\n",
    "\n",
    "# Retrieve list of available ONNX Runtime execution providers (e.g., CPU, CUDA, etc.)\n",
    "prov = ort.get_available_providers()\n",
    "\n",
    "# Print the available providers to verify GPU support\n",
    "print(prov)\n",
    "\n",
    "# Ensure that ONNX Runtime has access to GPU acceleration via CUDA\n",
    "# If not, raise an error with available providers listed\n",
    "assert \"CUDAExecutionProvider\" in prov, f\"ORT not GPU-enabled. Providers={prov}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a9a09-369f-4673-a16b-635331c76c8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Purpose\n",
    "\n",
    "Define key environment-based and hardcoded paths and model names used throughout the notebook or script. These paths help organize downloaded datasets and models for consistent storage and retrieval, while model_names lists the YOLO models to be processed.\n",
    "\n",
    "#### Context\n",
    "\n",
    "These variables are typically set up at the start of the workflow.\n",
    "\n",
    "- `datasets_dir` and `project_ds_folder` are pulled from Domino environment variables to ensure compatibility with the platform's file system.\n",
    "\n",
    "- `download_base_folder` combines them to point to the appropriate working directory.\n",
    "\n",
    "- `models_folder` specifies the subdirectory to store exported models.\n",
    "\n",
    "- `model_names` provides a list of YOLO model variants for batch processing or export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d38dd6-3f28-460c-b010-c38e13305fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasets_dir = os.environ['DOMINO_DATASETS_DIR']\n",
    "project_ds_folder = os.environ['DOMINO_PROJECT_NAME'] \n",
    "\n",
    "download_base_folder=f\"{datasets_dir}/{project_ds_folder}\"\n",
    "models_folder = \"models\"\n",
    "model_names = [\"yolov8n\", \"yolov5n\", \"yolov8m\", \"yolov8s\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadfdfc0-0ab6-4803-8419-bd87515d5536",
   "metadata": {},
   "source": [
    "#### Purpose\n",
    "\n",
    "Evaluates a YOLO-based object detection model on a COCO 2017 validation subset. It measures accuracy and latency, logs metrics to MLflow (if configured), and stores artifacts (plots, configs, and metrics) for reproducibility. This enables model-to-model comparisons on consistent datasets.\n",
    "\n",
    "#### Context\n",
    "\n",
    "Used in workflows where:\n",
    "\n",
    "- Multiple computer vision models (e.g., YOLOv5, YOLOv8) are benchmarked side-by-side.\n",
    "\n",
    "- Registered models (via MLflow Model Registry) or local model paths are evaluated.\n",
    "\n",
    "- Validation results need to be versioned and stored for audit and experiment tracking (Domino Experiment Manager)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aae6ad0-baf5-4f36-afc9-d09248480575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, time, yaml, math, random, json, shutil, statistics as stats\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Check for MLflow availability\n",
    "try:\n",
    "    import mlflow\n",
    "    _HAS_MLFLOW = True\n",
    "except Exception:\n",
    "    _HAS_MLFLOW = False\n",
    "\n",
    "# Ensures a directory exists; creates it if needed\n",
    "def _ensure_dir(p: Path) -> Path:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "# Run latency benchmarks for a given YOLO model on a list of images\n",
    "def _latency_benchmark(model: YOLO, image_paths: List[Path], imgsz: int, device: str) -> Dict[str, Any]:\n",
    "    if not image_paths:\n",
    "        return {\"p50_ms\": None, \"p90_ms\": None, \"p99_ms\": None, \"mean_ms\": None, \"count\": 0}\n",
    "\n",
    "    lat_ms = []\n",
    "    _ = model.predict(source=str(image_paths[0]), imgsz=imgsz, device=device, verbose=False)  # warm-up\n",
    "    for p in image_paths:\n",
    "        t0 = time.perf_counter()\n",
    "        _ = model.predict(source=str(p), imgsz=imgsz, device=device, verbose=False)\n",
    "        lat_ms.append((time.perf_counter() - t0) * 1000.0)\n",
    "\n",
    "    lat_sorted = sorted(lat_ms)\n",
    "    def pct(v, q):\n",
    "        idx = min(len(v)-1, max(0, int(math.ceil(q*len(v))-1)))\n",
    "        return v[idx]\n",
    "\n",
    "    return {\n",
    "        \"p50_ms\": pct(lat_sorted, 0.50),\n",
    "        \"p90_ms\": pct(lat_sorted, 0.90),\n",
    "        \"p99_ms\": pct(lat_sorted, 0.99),\n",
    "        \"mean_ms\": float(stats.mean(lat_sorted)),\n",
    "        \"count\": len(lat_sorted),\n",
    "        \"raw_ms\": lat_ms,\n",
    "    }\n",
    "\n",
    "# Evaluates a YOLO object detection model (from local path or MLflow registry),\n",
    "# computes accuracy and latency metrics, logs results to MLflow, and stores\n",
    "# all relevant artifacts for reproducibility.\n",
    "\n",
    "def evaluate_model(\n",
    "    base_path: str,\n",
    "    model_path: str = None,\n",
    "    imgsz: int = 640,\n",
    "    device: str = \"cpu\",\n",
    "    limit_images: int = 1000,\n",
    "    subset_seed: int = 0,\n",
    "    experiment_name: str = None,\n",
    "    registry_model_name: str = None,\n",
    "    registry_model_version: str = \"latest\",\n",
    "    parent_run_id: str = None,\n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    # Step 1: Set up input image directory\n",
    "    base = Path(base_path)\n",
    "    img_dir = base / \"images\" / \"val2017\"\n",
    "    if not img_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing image dir: {img_dir}\")\n",
    "\n",
    "    # Step 2: Set up output directories for artifacts\n",
    "    artifacts = _ensure_dir(base / \"artifacts\")\n",
    "    plots_dir = _ensure_dir(artifacts / \"plots\")\n",
    "    metrics_dir = _ensure_dir(artifacts / \"metrics\")\n",
    "    config_dir = _ensure_dir(artifacts / \"config\")\n",
    "\n",
    "    # Step 3: Load and shuffle images, apply subset limit\n",
    "    all_imgs = sorted([p for p in img_dir.glob(\"*.jpg\")] +\n",
    "                      [p for p in img_dir.glob(\"*.jpeg\")] +\n",
    "                      [p for p in img_dir.glob(\"*.png\")])\n",
    "    if not all_imgs:\n",
    "        raise RuntimeError(f\"No images found under {img_dir}\")\n",
    "    rng = random.Random(subset_seed)\n",
    "    rng.shuffle(all_imgs)\n",
    "    sub_imgs = all_imgs[:min(limit_images, len(all_imgs))]\n",
    "\n",
    "    # Step 4: Save subset to file for reproducibility\n",
    "    subset_list = artifacts / \"val_subset.txt\"\n",
    "    with open(subset_list, \"w\") as f:\n",
    "        for p in sub_imgs:\n",
    "            f.write(str(p.resolve()) + \"\\n\")\n",
    "\n",
    "    # Step 5: Write minimal COCO config YAML for Ultralytics\n",
    "    data_config = {\n",
    "        \"path\": str(base),\n",
    "        \"train\": \"images/val2017\",\n",
    "        \"val\": str(subset_list),\n",
    "        \"names\": list(range(80)),  # Dummy list of 80 class names\n",
    "    }\n",
    "    yaml_path = base / \"coco_val_subset.yaml\"\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.dump(data_config, f)\n",
    "\n",
    "    # Step 6: Load YOLO model from Registry or local path\n",
    "    if registry_model_name:\n",
    "        model = utilities.load_registered_yolo_model(registry_model_name, version=registry_model_version)\n",
    "        model_id = f\"{registry_model_name}:{registry_model_version}\"\n",
    "    else:\n",
    "        if not model_path:\n",
    "            raise ValueError(\"Provide either registry_model_name or model_path\")\n",
    "        model = YOLO(model_path, task=\"detect\")\n",
    "        model_id = Path(model_path).stem\n",
    "\n",
    "    # Step 7: Set up Ultralytics output folder for the run\n",
    "    tmp_project = Path(\"/tmp/ultra_runs\")\n",
    "    run_name = f\"val_{model_id}_subset{len(sub_imgs)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "    # Step 8: Run validation (metrics: mAP, precision, recall, etc.)\n",
    "    val_res = model.val(\n",
    "        data=str(yaml_path),\n",
    "        imgsz=imgsz,\n",
    "        device=device,\n",
    "        save_json=False,\n",
    "        verbose=False,\n",
    "        project=str(tmp_project),\n",
    "        name=run_name,\n",
    "        exist_ok=True,\n",
    "        workers=0,\n",
    "    )\n",
    "    save_dir = Path(val_res.save_dir)\n",
    "\n",
    "    # Step 9: Gather performance metrics\n",
    "    metrics = {\n",
    "        \"map\": float(val_res.box.map),\n",
    "        \"ap50\": float(val_res.box.map50),\n",
    "        \"ap75\": float(val_res.box.map75),\n",
    "        \"mean_precision\": float(val_res.box.mp),\n",
    "        \"mean_recall\": float(val_res.box.mr),\n",
    "        \"evaluated_images\": len(sub_imgs),\n",
    "        \"model_id\": model_id,\n",
    "    }\n",
    "    (metrics_dir / \"headline.json\").write_text(json.dumps(metrics, indent=2))\n",
    "\n",
    "    # Step 10: Run latency benchmark (single-image inference timing)\n",
    "    try:\n",
    "        latency = _latency_benchmark(model, sub_imgs[:min(100, len(sub_imgs))], imgsz=imgsz, device=device)\n",
    "    except NameError:\n",
    "        latency = {}\n",
    "    (metrics_dir / \"latency.json\").write_text(json.dumps({k: v for k, v in latency.items() if k != \"raw_ms\"}, indent=2))\n",
    "\n",
    "    # Step 11: Plot latency histogram (if available)\n",
    "    if latency.get(\"raw_ms\"):\n",
    "        plt.figure()\n",
    "        plt.hist(latency[\"raw_ms\"], bins=20)\n",
    "        plt.xlabel(\"Latency (ms)\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.title(\"Per-image latency (batch=1)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plots_dir / \"latency_hist.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # Step 12: Save run config for traceability\n",
    "    eval_cfg = {\n",
    "        \"imgsz\": imgsz,\n",
    "        \"device\": device,\n",
    "        \"subset_seed\": subset_seed,\n",
    "        \"limit_images\": limit_images,\n",
    "        \"subset_list\": str(subset_list),\n",
    "        \"model_id\": model_id,\n",
    "        \"ultralytics_save_dir\": str(save_dir),\n",
    "        \"source\": \"registry\" if registry_model_name else \"path\",\n",
    "    }\n",
    "    (config_dir / \"eval.json\").write_text(json.dumps(eval_cfg, indent=2))\n",
    "\n",
    "    # Step 13: Log everything to MLflow (if enabled)\n",
    "    if experiment_name:\n",
    "        utilities.ensure_mlflow_experiment(experiment_name)\n",
    "\n",
    "    if experiment_name or parent_run_id:\n",
    "        try:\n",
    "            with mlflow.start_run(run_name=run_name, nested=bool(parent_run_id)):\n",
    "                mlflow.set_tags({\"model_id\": model_id, \"source\": eval_cfg[\"source\"]})\n",
    "                mlflow.log_params({\n",
    "                    \"imgsz\": imgsz,\n",
    "                    \"device\": device,\n",
    "                    \"limit_images\": len(sub_imgs),\n",
    "                    \"subset_seed\": subset_seed\n",
    "                })\n",
    "                mlflow.log_metrics({\n",
    "                    \"map\": metrics[\"map\"],\n",
    "                    \"ap50\": metrics[\"ap50\"],\n",
    "                    \"ap75\": metrics[\"ap75\"],\n",
    "                    \"mean_precision\": metrics[\"mean_precision\"],\n",
    "                    \"mean_recall\": metrics[\"mean_recall\"],\n",
    "                    \"latency_p50_ms\": latency.get(\"p50_ms\", 0.0),\n",
    "                    \"latency_p90_ms\": latency.get(\"p90_ms\", 0.0),\n",
    "                    \"latency_p99_ms\": latency.get(\"p99_ms\", 0.0),\n",
    "                    \"latency_mean_ms\": latency.get(\"mean_ms\", 0.0),\n",
    "                })\n",
    "                mlflow.log_artifact(str(metrics_dir / \"headline.json\"))\n",
    "                mlflow.log_artifact(str(metrics_dir / \"latency.json\"))\n",
    "                mlflow.log_artifact(str(config_dir / \"eval.json\"))\n",
    "                mlflow.log_artifact(str(artifacts / \"val_subset.txt\"))\n",
    "                if (plots_dir / \"latency_hist.png\").exists():\n",
    "                    mlflow.log_artifact(str(plots_dir / \"latency_hist.png\"))\n",
    "                if save_dir.exists():\n",
    "                    mlflow.log_artifacts(str(save_dir), artifact_path=\"ultralytics_run\")\n",
    "        finally:\n",
    "            # Step 14: Clean up temporary run folders\n",
    "            if save_dir.exists():\n",
    "                shutil.rmtree(save_dir, ignore_errors=True)\n",
    "            parent = tmp_project\n",
    "            if parent.exists() and not any(parent.iterdir()):\n",
    "                shutil.rmtree(parent, ignore_errors=True)\n",
    "    else:\n",
    "        if save_dir.exists():\n",
    "            shutil.rmtree(save_dir, ignore_errors=True)\n",
    "\n",
    "    # Step 15: Return headline metrics to caller\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfa42b-d533-4419-9f83-976b6aa8e39c",
   "metadata": {},
   "source": [
    "### Benchmarking Workflow Overview\n",
    "\n",
    "This benchmarking flow enables consistent and reproducible evaluation of multiple YOLO models using the COCO dataset and MLflow for experiment tracking.\n",
    "\n",
    "## Workflow Steps\n",
    "\n",
    "1. Start a Parent MLflow Run\n",
    "\n",
    "- All evaluations are grouped under a single parent run.\n",
    "\n",
    "- Helps organize metrics and artifacts across models for easy comparison.\n",
    "\n",
    "2. Iterate Through YOLO Model Variants\n",
    "\n",
    "- Each model (e.g., yolov8n, yolov5n, etc.) is loaded from the MLflow Model Registry.\n",
    "\n",
    "- Evaluation is performed using a controlled subset of validation images.\n",
    "\n",
    "3. Call evaluate_model()\n",
    "   For each model:\n",
    "   - Runs val() using Ultralytics YOLO.\n",
    "\n",
    "   - Collects accuracy metrics (e.g., mAP, AP50, precision, recall).\n",
    "\n",
    "   - Benchmarks inference latency on the selected device.\n",
    "\n",
    "   - Logs results to MLflow as a child run.\n",
    "\n",
    "4. Log Metrics and Artifacts\n",
    "\n",
    "   Each child run logs:\n",
    "\n",
    "   - Evaluation metrics (JSON)\n",
    "\n",
    "   - Subset configuration (TXT)\n",
    "\n",
    "   - Latency histograms (PNG)\n",
    "\n",
    "   - Ultralytics output artifacts\n",
    "\n",
    "5. Reproducibility by Design\n",
    "\n",
    "   - Uses fixed random seed (subset_seed) and image limit to ensure consistent subsets.\n",
    "\n",
    "   - Results are isolated and traceable per model while remaining linked to the parent.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Isolated runs**: Each model evaluation is self-contained and trackable.\n",
    "\n",
    "- **Fair comparison**: All models are evaluated on the same image subset.\n",
    "\n",
    "- **Re-runnable**: Deterministic behavior allows rerunning or adding new models easily.\n",
    "\n",
    "- **Experiment visibility**: MLflow UI allows side-by-side metric comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03455c5b-60c5-4afa-b281-ef37e790fed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating yolov8n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loaded] yolov8n:latest from /tmp/yolov8n_fkrlu_ro/model.onnx\n",
      "Ultralytics 8.3.182 ðŸš€ Python-3.8.10 torch-2.3.1+cu121 CUDA:0 (NVIDIA A10G, 22724MiB)\n",
      "Loading /tmp/yolov8n_fkrlu_ro/model.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CUDAExecutionProvider\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/home/domino/.config/Ultralytics/Arial.ttf': 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 755k/755k [00:00<00:00, 23.5MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.3Â±0.4 ms, read: 14.6Â±3.1 MB/s, size: 145.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/data/reference-cv-model-comparison/coco/labels/val2017.cache... 50 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:04<00:00, 11.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50        564      0.529      0.531      0.585      0.446\n",
      "Speed: 2.8ms preprocess, 4.0ms inference, 0.0ms loss, 28.9ms postprocess per image\n",
      "Results saved to \u001b[1m/tmp/ultra_runs/val_yolov8n:latest_subset50_20250822_150209\u001b[0m\n",
      "Loading /tmp/yolov8n_fkrlu_ro/model.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CUDAExecutionProvider\n",
      "Evaluating yolov5n\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:01<00:00,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loaded] yolov5n:latest from /tmp/yolov5n_zrj_pzbe/model.onnx\n",
      "Ultralytics 8.3.182 ðŸš€ Python-3.8.10 torch-2.3.1+cu121 CUDA:0 (NVIDIA A10G, 22724MiB)\n",
      "Loading /tmp/yolov5n_zrj_pzbe/model.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CUDAExecutionProvider\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 96.7Â±33.5 MB/s, size: 153.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/data/reference-cv-model-comparison/coco/labels/val2017.cache... 50 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 50.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50        564       0.59      0.521       0.56      0.419\n",
      "Speed: 0.3ms preprocess, 3.9ms inference, 0.0ms loss, 6.7ms postprocess per image\n",
      "Results saved to \u001b[1m/tmp/ultra_runs/val_yolov5n:latest_subset50_20250822_150233\u001b[0m\n",
      "Loading /tmp/yolov5n_zrj_pzbe/model.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CUDAExecutionProvider\n",
      "Evaluating yolov8m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loaded] yolov8m:latest from /tmp/yolov8m_4xy1d64h/model.onnx\n",
      "Ultralytics 8.3.182 ðŸš€ Python-3.8.10 torch-2.3.1+cu121 CUDA:0 (NVIDIA A10G, 22724MiB)\n",
      "Loading /tmp/yolov8m_4xy1d64h/model.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CUDAExecutionProvider\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.6Â±0.0 ms, read: 79.1Â±12.4 MB/s, size: 111.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/data/reference-cv-model-comparison/coco/labels/val2017.cache... 50 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:01<00:00, 48.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50        564      0.635      0.675      0.687      0.547\n",
      "Speed: 0.6ms preprocess, 9.7ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/tmp/ultra_runs/val_yolov8m:latest_subset50_20250822_150252\u001b[0m\n",
      "Loading /tmp/yolov8m_4xy1d64h/model.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CUDAExecutionProvider\n",
      "Evaluating yolov8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loaded] yolov8s:latest from /tmp/yolov8s_xas6gmz6/model.onnx\n",
      "Ultralytics 8.3.182 ðŸš€ Python-3.8.10 torch-2.3.1+cu121 CUDA:0 (NVIDIA A10G, 22724MiB)\n",
      "Loading /tmp/yolov8s_xas6gmz6/model.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CUDAExecutionProvider\n",
      "Setting batch=1 input of shape (1, 3, 640, 640)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 117.0Â±51.6 MB/s, size: 151.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/data/reference-cv-model-comparison/coco/labels/val2017.cache... 50 images, 2 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 58.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         50        564      0.639       0.61      0.652      0.498\n",
      "Speed: 0.6ms preprocess, 6.6ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1m/tmp/ultra_runs/val_yolov8s:latest_subset50_20250822_150318\u001b[0m\n",
      "Loading /tmp/yolov8s_xas6gmz6/model.onnx for ONNX Runtime inference...\n",
      "Using ONNX Runtime CUDAExecutionProvider\n"
     ]
    }
   ],
   "source": [
    "#I needed to increase the shared memory usage to 10GB\n",
    "domino_user_name = os.environ['DOMINO_USER_NAME']\n",
    "experiment_name=f\"cv-benchmark-{domino_user_name}\"\n",
    "base_folder=f\"{download_base_folder}/coco\"\n",
    "\n",
    "utilities.ensure_mlflow_experiment(experiment_name)\n",
    "\n",
    "\n",
    "base_path=f\"{download_base_folder}/coco\"\n",
    "imgsz=640\n",
    "device=\"0\"\n",
    "limit_images=50\n",
    "subset_seed=0\n",
    "\n",
    "with mlflow.start_run(run_name=f\"parent_benchmark_{limit_images}\") as parent:\n",
    "    parent_id = parent.info.run_id\n",
    "    mlflow.log_params({\n",
    "                        \"base_path\":base_path,\n",
    "                        \"imgsz\": imgsz, \"device\": device,\n",
    "                        \"limit_images\": limit_images, \"subset_seed\": subset_seed\n",
    "                    })\n",
    "    for model_name in model_names:    \n",
    "        print(f\"Evaluating {model_name}\")\n",
    "        evaluate_model(\n",
    "            base_path=base_path,\n",
    "            registry_model_name=model_name,\n",
    "            registry_model_version=\"latest\",\n",
    "            imgsz=imgsz,\n",
    "            device=device,\n",
    "            limit_images=limit_images,\n",
    "            subset_seed=subset_seed,\n",
    "            experiment_name=None,          # experiment already set\n",
    "            parent_run_id=parent_id,       # pass parent\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e4edffa-0a41-4389-b471-ab31f0c9ae47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#metrics = evaluate_model(f\"{download_base_folder}/coco\",model_path,limit_images=50,experiment_name=experiment_name,device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2b21b-49dc-4a81-8a9f-453e39e6a341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
